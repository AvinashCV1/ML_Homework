#Task1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re

from sklearn import tree

from IPython.display import Image as PImage
from subprocess import check_call
from PIL import Image, ImageDraw, ImageFont
from collections import Counter

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import scipy.integrate as integrate
import scipy

data = pd.read_csv('/data.csv')
labels = pd.read_csv('/label.csv',names=['label'],header=None)
df_null = round(100*(data.isnull().sum())/len(data), 2)
data_df=data.dropna()

#Elbow Curve to get the right number of Clusters
ssd = []
range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9,10, 11, 12]
for num_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)
    kmeans.fit(data)
    
    ssd.append(kmeans.inertia_)
    
# plot the SSDs for each n_clusters
plt.plot(ssd)


#Model Building
kmeans = KMeans(n_clusters=4, max_iter=50)
kmeans.fit(data)

#Split data
from sklearn.model_selection import train_test_split
train_data, test_data = train_test_split( data, test_size=0.08, random_state=42)
train_labels, test_labels = train_test_split( labels, test_size=0.08, random_state=42)

class KMeans:
    
    def calculate_SSE(self, centroid_value_dict, centroid_dict,data):
        sse_data = 0
        for i in centroid_dict:
            sse_cluster = 0
            # np.sum()
            for j in centroid_dict[i]:
                dp = list(data.iloc[int(j)])
                for a,b in zip(centroid_value_dict[i],dp):
                    sse_cluster += (a-b)**2
            sse_data+=sse_cluster
        return sse_data    
    
    def Initialize_Centroids(self,data,K):
        m = data.shape[0]
        centroid_value_dict={}
        for i in range(K):
            r = np.random.randint(0, m-1)
            centroid_value_dict[i] = data.iloc[r]
        return centroid_value_dict
    
    def jaccard_similarity(self,centroid, dp):
        intersection = len(list(set(centroid).intersection(dp)))
        union = (len(set(centroid)) + len(set(dp))) - intersection
        return float(intersection) / union

    def train_Kmeans(self,data,K,max_iter=20,mode=1,tol=10):
        #Mode = 1 => Euclidean np.linalg.norm(x-list(data.iloc[i,:]))
        #Mode = 2 => Jaccard
        #Mode = 3 => Cosine
        centroid_value_dict = self.Initialize_Centroids(data,K)
        new_centroid_value_dict = {}
        count = 0
        centroid_dict = {}
        convergence = False
        while((count<max_iter) and not convergence):
            
            for i in list(centroid_value_dict.keys()):
                centroid_dict[i]=[]
            for i in range(data.shape[0]):
                x = data.iloc[i]
                if mode==1 :
                    distance_measure = [np.linalg.norm(x-centroid_value_dict[j])  for j in centroid_value_dict]
                    idx = np.argmin(distance_measure)
                    centroid_dict[idx].append(i)
                elif mode==2 :
                    distance_measure = [self.jaccard_similarity(list(x),centroid_value_dict[j]) for j in centroid_value_dict]
                    idx = np.argmax(distance_measure)
                    centroid_dict[idx].append(i)
                elif mode==3 :
                    distance_measure = [1-scipy.spatial.distance.cosine(x,list(centroid_value_dict[j]))  for j in centroid_value_dict]
                    idx = np.argmax(distance_measure)
                    centroid_dict[idx].append(i)
                
                prev_centroids=dict(centroid_value_dict)
                
            
            for i in centroid_dict:
                if len(centroid_dict[i]):
                    dps_centroid = centroid_dict[i]
                    centroid_value_dict[i] = np.average(data.iloc[dps_centroid],axis=0)
            
            
            current_tol=-1
            for i in centroid_value_dict:
                prev_centroid_point = prev_centroids[i]
                new_centroid_point = centroid_value_dict[i]
                change = np.sum(np.absolute(new_centroid_point-prev_centroid_point))
                current_tol = max(change, current_tol)
                
            print("Tolerance for the Iteration ",count,": ",current_tol)
            
            count+=1
            if (current_tol<10):
                convergence = True
                break
           # print("KMeans Iteration",count)
        return centroid_value_dict,centroid_dict
    
def predict_cluster_labels(C, S, labels):
    '''
    Input : C -> Centroids
            S -> Set of Indicies corresponding to Centroid C
            data -> Data used to form clusters
    Output : Returns an array of size K having labels based on majority voting in the cluster
    '''
    cluster_labels = np.zeros(10,dtype=int)
    for c in C:
        labels_of_points = []
        for point in S[c]:
            labels_of_points.extend(labels.iloc[point])
        counter = Counter(labels_of_points)
        try:
            cluster_labels[c] = max(counter, key=counter.get)
        except:
            cluster_labels[c] = np.random.randint(0,9)
    return cluster_labels

def jaccard_similarity(centroid, dp):
        intersection = len(list(set(centroid).intersection(dp)))
        union = (len(set(centroid)) + len(set(dp))) - intersection
        return float(intersection) / union

def accuracy(centroids, centroid_Labels, test_data, true_labels, mode=1):
    y_true = list(true_labels['label']);
    y_pred = []
    for index in range(test_data.shape[0]):
        featureset = test_data.iloc[index]
        if mode==1:
            distances = [np.linalg.norm(featureset - centroids[centroid]) for centroid in centroids]
            classification = distances.index(min(distances))
            y_pred.append(centroid_Labels[classification])
        elif mode==2:
            similarity = [jaccard_similarity(featureset, centroids[centroid]) for centroid in centroids]
            classification = similarity.index(max(similarity))
            y_pred.append(centroid_Labels[classification]) 
        elif mode==3:
            similarity = [1 - spatial.distance.cosine(featureset, centroids[centroid]) for centroid in centroids]
            classification = similarity.index(max(similarity))
            y_pred.append(centroid_Labels[classification])
    denominator = test_data.shape[0]
    correctly_classified = 0
    for i in range(0,len(y_pred)):
        if y_true[i] == y_pred[i]:
            correctly_classified += 1
    accuracy = correctly_classified/denominator
    return accuracy

model1 = KMeans()
centroids1,clusters1 = model1.train_Kmeans(data,10, max_iter=100,mode=1)

Euclidean_SSE = model1.calculate_SSE(centroids1,clusters1,data)


# In[66]:


print("Euclidean SSE:",Euclidean_SSE)


# In[67]:


cluster_labels1 = predict_cluster_labels(centroids1,clusters1,labels)
cluster_labels1


# In[68]:


Accuracy_Euclidean = accuracy(centroids1, cluster_labels1,test_data,test_labels)
Accuracy_Euclidean


# In[69]:


model2 = KMeans()
centroids2,clusters2 = model2.train_Kmeans(data,10, max_iter=100,mode=2)
Jaccard_SSE = model2.calculate_SSE(centroids2,clusters2,data)


# In[70]:


print("Jacard SSE:",Jaccard_SSE)


# In[71]:


cluster_labels2 = predict_cluster_labels(centroids2,clusters2,labels)
cluster_labels2


# In[72]:


Accuracy_Jaccard = accuracy(centroids2, cluster_labels2,test_data,test_labels)
Accuracy_Jaccard


# In[77]:


model3 = KMeans()
centroids3,clusters3 = model3.train_Kmeans(data,10, max_iter = 100,mode=3)


# In[85]:


Cosine_SSE = model3.calculate_SSE(centroids3,clusters3,data)


# In[90]:


cluster_labels3 = predict_cluster_labels(centroids3,clusters3,labels)
cluster_labels3


# In[91]:


Accuracy_Cosine = accuracy(centroids3, cluster_labels3,test_data,test_labels)
Accuracy_Cosine


# In[92]:


print("Euclidean SSE:",Euclidean_SSE)
print("Jacard SSE:",Jaccard_SSE)
print("Cosine SSE :",Cosine_SSE)


# In[93]:


cluster_labels3 = predict_cluster_labels(centroids3,clusters3,labels)
cluster_labels3


# In[94]:


#Accuracy_Cosine = accuracy(centroids3, cluster_labels3,test_data,test_labels)
Accuracy_Euclidean
Accuracy_Jaccard
Accuracy_Cosine


# In[95]:


print("Euclidean accuracy:",Accuracy_Euclidean)
print("Jacard accuracy:",Accuracy_Jaccard)
print("Cosine accuracy :",Accuracy_Cosine)


#Task2
#Download surprise lib
import sys
!{sys.executable} -m pip install scikit-surprise
import pandas as pd
import numpy as np
from surprise import SVD
import numpy as np
import surprise
from surprise import Reader, Dataset
from surprise.model_selection import cross_validate
from surprise import KNNBasic
from matplotlib import pyplot as plt

#Read the dataset
ratings_df = pd.read_csv('ratings_small.csv')
ratings_df.head()
ratings_reader = Reader(line_format='user item rating timestamp',sep=',',skip_lines=1, rating_scale=(1, 5))
ratings_data = Dataset.load_from_file("ratings_small.csv", ratings_reader)
ratings_reader = Reader(line_format='user item rating timestamp',sep=',',skip_lines=1, rating_scale=(1, 5))
ratings_data = Dataset.load_from_file("ratings_small.csv", ratings_reader)
PMF_info = cross_validate(SVD(biased = False), ratings_data, measures=['RMSE', 'MAE'], cv=5, verbose = True)
print("PMF info -", PMF_info)
PMF_infos_data = pd.DataFrame.from_dict(PMF_info)
print("Average PMF MAE:", round(PMF_infos_data['test_mae'].mean(), 2))
print("Average PMF RMSE:", round(PMF_infos_data['test_rmse'].mean(), 2))
#Using KNNbasis
'''Setting user_based to true for user-based collabrative filtering'''
sim_options = {'user_based': True }
user_based = KNNBasic(sim_options=sim_options)
user_based_CF_results = cross_validate(user_based,ratings_data,measures=['rmse', 'mae'],cv=5,verbose=True)
print('user based results -', user_based_CF_results)
user_based_CF_results_data = pd.DataFrame.from_dict(user_based_CF_results)
print("Average User Based CF MAE:", round(user_based_CF_results_data['test_mae'].mean(), 2))
print("Average User Based CF RMSE:", round(user_based_CF_results_data['test_rmse'].mean(), 2))

#MAE for User Based for all similarities
label = ['User Based']
value = np.arange(len(label))  # the label locations

fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.bar(value + 0.00, graph_data['mae']['cosine'][0], color = 'c', width = 0.25)
ax.bar(value + 0.25, graph_data['mae']['msd'][0], color = 'm', width = 0.25)
ax.bar(value + 0.50, graph_data['mae']['pearson_baseline'][0], color = 'y', width = 0.25)

plt.xticks(value, label)
plt.ylabel("MAE")
plt.yticks()
plt.legend(["Cosine", "MSD", "Pearson"])
plt.show()

#MAE for Item Based for all similarities
label = ['Item Based']
value = np.arange(len(label))  # the label locations


fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.bar(value + 0.00, graph_data['mae']['cosine'][1], color = 'c', width = 0.25)
ax.bar(value + 0.25, graph_data['mae']['msd'][1], color = 'm', width = 0.25)
ax.bar(value + 0.50, graph_data['mae']['pearson_baseline'][1], color = 'y', width = 0.25)

plt.xticks(value, label)
plt.ylabel("MAE")
plt.yticks()
plt.legend(["Cosine", "MSD", "Pearson"])
plt.show()
